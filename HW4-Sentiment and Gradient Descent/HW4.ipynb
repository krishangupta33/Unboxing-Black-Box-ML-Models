{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Problem 1\n",
    "<img src=\"problem1.png\" width=\"800\" height=\"600\" alt=\"Problem 1 Image\">  -->\n",
    "\n",
    "### Problem 1.\n",
    "\n",
    "Consider the following loss function $L(\\beta)$ where $(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$ are the observations in our dataset. $x_i$ is a $d$-dimensional input vector, i.e., there are $d$ features in our dataset. So, $x_{ij}$ corresponds to the $j^{th}$ feature in the $i^{th}$ observation. $y_i$ corresponds to the outcome variable for observation $i$. $\\lambda$ is some scalar constant.\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{d} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{d} \\beta_j^2\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1\n",
    "\n",
    "### (i) Estimating the parameters $\\beta$ analytically as we did for linear regression\n",
    "\n",
    "The loss function provided is a ridge regression loss function, which is used for linear regression with \n",
    "L2 regularization. The L2 regularization is included to prevent overfitting by penalizing large coefficients through the regularization parameter\n",
    "λ.\n",
    "\n",
    "\n",
    "Analytically solving for the parameters $\\beta$ involves finding the values of $\\beta$ that minimize the loss function $L(\\beta)$. For ordinary linear regression without regularization (i.e., when $\\lambda = 0$), this can be done by setting the derivative of the loss function with respect to each $\\beta_j$ to zero and solving the resulting normal equations. However, with the addition of the L2 regularization term, the solution is not the same as ordinary least squares (OLS) because of the penalty on the size of the coefficients.\n",
    "\n",
    "To find the analytical expression for the $\\beta$ parameters in ridge regression, we also set the derivative of $L(\\beta)$ with respect to each $\\beta_j$ to zero. This will give us a set of equations that we can solve for $\\beta$.\n",
    "\n",
    "Let's denote our design matrix as $X$ (with each row corresponding to an observation and each column to a feature) and our response vector as $y$. The loss function can be written in matrix form as:\n",
    "\n",
    "$$ L(\\beta) = (y - X\\beta)^T(y - X\\beta) + \\lambda\\beta^T\\beta $$\n",
    "\n",
    "To minimize this loss function, we take the derivative with respect to $\\beta$ and set it to zero:\n",
    "\n",
    "$$ \\frac{\\partial L(\\beta)}{\\partial \\beta} = -2X^T(y - X\\beta) + 2\\lambda\\beta = 0 $$\n",
    "\n",
    "This gives us the ridge regression normal equations:\n",
    "\n",
    "$$ X^TX\\beta + \\lambda I\\beta = X^Ty $$\n",
    "\n",
    "where $I$ is the identity matrix. The solution to this equation is:\n",
    "\n",
    "$$ \\beta = (X^TX + \\lambda I)^{-1}X^Ty $$\n",
    "\n",
    "----------------\n",
    "### (ii) Gradient of the loss function $\\nabla L(\\beta)$ \n",
    "\n",
    "$$\\nabla L(\\beta) = -2X^T(y - X\\beta) + 2\\lambda\\beta $$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- $X$ is the matrix of input features, with rows representing samples and columns representing features.\n",
    "- $X^T$ is the transpose of the matrix $X$.\n",
    "- $y$ is the vector of observed values (target values).\n",
    "- $\\beta$ is the vector of parameters that we are trying to learn.\n",
    "- $\\lambda$ is the regularization parameter that controls the amount of shrinkage: larger values of $\\lambda$ shrink the parameters more toward zero.\n",
    "\n",
    "----------------\n",
    "\n",
    "### (iii) Update step for gradient descent\n",
    "\n",
    "$$  \\beta_j^{(t+1)} = \\beta_j^{(t)} - \\eta \\cdot \\frac{\\partial L(\\beta)}{\\partial \\beta_j} $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\beta_j^{(t+1)}$ is the updated value of the $j$-th parameter at iteration $t+1$.\n",
    "- $\\beta_j^{(t)}$ is the current value of the $j$-th parameter at iteration $t$.\n",
    "- $\\eta$ is the learning rate, a positive scalar determining the step size at each iteration.\n",
    "- $\\frac{\\partial L(\\beta)}{\\partial \\beta_j}$ is the partial derivative of the loss function with respect to the $j$-th parameter, representing the direction and rate of the steepest increase in the loss function.\n",
    "\n",
    "By subtracting the gradient scaled by the learning rate from the current parameters, the update rule moves the parameters in the direction that most steeply reduces the loss function.\n",
    "\n",
    "----------------\n",
    "### (iv) pseudo-code for a stochastic gradient descent (SGD) algorithm to estimate parameters $\\beta$ \n",
    "\n",
    "- Initialize β at random\n",
    "- Choose a learning rate η\n",
    "\n",
    "- Repeat the following until an approximate minimum is obtained:\n",
    "    - Shuffle the dataset randomly\n",
    "    - For each example in the dataset:\n",
    "        - Calculate the gradient of the loss with respect to the example\n",
    "        - Update β by subtracting η times the gradient from β\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------\n",
    "## ----------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Twiter Sentiment Analysis\n",
    "\n",
    "Dataset Link: https://www.kaggle.com/datasets/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/krishan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/krishan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/krishan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#NLTK for tweet(text) processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#TF-IDF (Term Frequency-Inverse Document Frequency: Vectorizer)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "\n",
    "Original file of 1.6 million tweets was used to create a smaller dataset of 2000 tweets. Following Codeblock has been commented out as I will be using the processed file with 2000 tweeets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('tweets.csv', encoding='latin-1')\n",
    "# df.head()\n",
    "\n",
    "# #drop all except column 0 and last\n",
    "# df = df.iloc[:,[0,-1]]\n",
    "# #rename first to target, last to tweeet\n",
    "# df.columns = ['target','tweet']\n",
    "# df.head()\n",
    "\n",
    "# #sample random 2000 rows\n",
    "# df0 = df[df['target']==0].sample(1000)\n",
    "# df4 = df[df['target']==4].sample(1000)\n",
    "\n",
    "# #concat the data\n",
    "# df = pd.concat([df0, df4], ignore_index=True) \n",
    "\n",
    "# #shuffle the data\n",
    "# df = df.sample(frac=1)\n",
    "\n",
    "# df['target'].value_counts()\n",
    "\n",
    "# #change the target to 0 and 1\n",
    "# df['target'] = df['target'].replace(4,1)\n",
    "\n",
    "# #switching the columns\n",
    "# df = df[['tweet','target']]\n",
    "\n",
    "# #save to csv\n",
    "# df.to_csv('tweets_2000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Back from drankin...now going to the movies w ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@SwayShay I know, and LD is too</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boohoo  Have to wait until 1 PM to download iP...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My internet is mega slow again and ruining my ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thinking of ideas and inspiration for raspberr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  target\n",
       "0  Back from drankin...now going to the movies w ...       1\n",
       "1                   @SwayShay I know, and LD is too        0\n",
       "2  Boohoo  Have to wait until 1 PM to download iP...       0\n",
       "3  My internet is mega slow again and ruining my ...       0\n",
       "4  thinking of ideas and inspiration for raspberr...       1"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the data\n",
    "df = pd.read_csv('tweets_2000.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet     0\n",
      "target    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    1000\n",
       "0    1000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check target proportion\n",
    "df['target'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "962     ...weather today. Going out for lunch with my ...\n",
       "984     @gargamit100 laugh at your win this time; next...\n",
       "992      I want to go to sleep but youtube is being slow \n",
       "1203            Ubertwitter still not updating on the bb \n",
       "1031    Tonto and wera.. luv u both.. u really make me...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train test split\n",
    "X = df['tweet']\n",
    "y = df['target']\n",
    "\n",
    "#startify the split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "1    800\n",
      "0    800\n",
      "Name: count, dtype: int64\n",
      "target\n",
      "0    200\n",
      "1    200\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check target proportion in train and test\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "962     weather today going lunch mummy going orthodon...\n",
       "984     gargamit laugh win time next time junior give ...\n",
       "992                            want go sleep youtube slow\n",
       "1203                        ubertwitter still updating bb\n",
       "1031                 tonto wera luv u u really make happy\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing function to clean text data\n",
    "def clean_text(text):\n",
    "    # Remove non-alphabetic characters and lowercase the text\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Join the tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# lametize the words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# Apply the preprocessing function to the text data\n",
    "X_train = X_train.apply(clean_text)\n",
    "X_train = X_train.apply(lemmatize_words)\n",
    "X_test = X_test.apply(clean_text)\n",
    "X_test = X_test.apply(lemmatize_words)\n",
    "\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600,), (400,))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 4814), (400, 4814))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making word Embeddings Using TF-IDF\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Fit the vectorizer to the training data\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Vectorization has been used to convert the text data into numerical data. Now each tweet is represented by a 4814 dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaaah</th>\n",
       "      <th>aaahhh</th>\n",
       "      <th>aaarrggghhh</th>\n",
       "      <th>aaaw</th>\n",
       "      <th>aag</th>\n",
       "      <th>aahhh</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>...</th>\n",
       "      <th>zombecca</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoooooooooo</th>\n",
       "      <th>zrlgrl</th>\n",
       "      <th>zu</th>\n",
       "      <th>zvgd</th>\n",
       "      <th>zwinky</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4814 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaaaaah  aaahhh  aaarrggghhh  aaaw  aag  aahhh  aaron   ab  abandoned  abc  \\\n",
       "0      0.0     0.0          0.0   0.0  0.0    0.0    0.0  0.0        0.0  0.0   \n",
       "1      0.0     0.0          0.0   0.0  0.0    0.0    0.0  0.0        0.0  0.0   \n",
       "2      0.0     0.0          0.0   0.0  0.0    0.0    0.0  0.0        0.0  0.0   \n",
       "3      0.0     0.0          0.0   0.0  0.0    0.0    0.0  0.0        0.0  0.0   \n",
       "4      0.0     0.0          0.0   0.0  0.0    0.0    0.0  0.0        0.0  0.0   \n",
       "\n",
       "   ...  zombecca  zombie  zoo  zoom  zoooooooooo  zrlgrl   zu  zvgd  zwinky  \\\n",
       "0  ...       0.0     0.0  0.0   0.0          0.0     0.0  0.0   0.0     0.0   \n",
       "1  ...       0.0     0.0  0.0   0.0          0.0     0.0  0.0   0.0     0.0   \n",
       "2  ...       0.0     0.0  0.0   0.0          0.0     0.0  0.0   0.0     0.0   \n",
       "3  ...       0.0     0.0  0.0   0.0          0.0     0.0  0.0   0.0     0.0   \n",
       "4  ...       0.0     0.0  0.0   0.0          0.0     0.0  0.0   0.0     0.0   \n",
       "\n",
       "   zzzz  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   0.0  \n",
       "4   0.0  \n",
       "\n",
       "[5 rows x 4814 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i) Features I am using in logistic regression model with mathematical model\n",
    "\n",
    "I used TF-IDF (Term Frequency-Inverse Document Frequency) as a feature extraction method for a logistic regression model.\n",
    "\n",
    "\n",
    "The features are numerical representations of the importance of words (or terms) within dataset's document(here each row is one document) relative to a collection of documents. The TF-IDF score reflects how important a word is to a document in a collection or corpus. This method helps in distinguishing the significance of words in a document, considering their frequency across the entire corpus.\n",
    "\n",
    "### Features in Your Model\n",
    "- **TF (Term Frequency):** Measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n",
    "  \n",
    "  $$ TF(t) = \\frac{\\text{Number of times term } t \\text{ appears in a document}}{\\text{Total number of terms in the document}} $$\n",
    "\n",
    "- **IDF (Inverse Document Frequency):** Measures how important a term is. While computing TF, all terms are considered equally important. However, certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus, we need to weigh down the frequent terms while scaling up the rare ones, by computing the following:\n",
    "\n",
    "  $$ IDF(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents with term } t \\text{ in it}}\\right) $$\n",
    "  \n",
    "By multiplying TF and IDF, we get the TF-IDF score of a term in a document, which reflects the importance of the term in the document out of the whole corpus of documents. The higher the TF-IDF score, the more important the term is in that particular document.\n",
    "\n",
    "### Mathematical Representation of Logistic Regression Model Using TF-IDF Features\n",
    "\n",
    "Given that features $X$ are created using TF-IDF, each document $d$ in training data is represented as a vector $X_d = [x_1, x_2, ..., x_n]$, where each $x_i$ is the TF-IDF score for term $i$ in document $d$, and $n$ is the total number of unique terms across all documents in the corpus.\n",
    "\n",
    "The logistic regression model can then be mathematically represented as follows:\n",
    "\n",
    "$$ P(y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n)}} $$\n",
    "\n",
    "Where:\n",
    "- $P(y=1|X)$ is the probability that the document belongs to class 1 (assuming a binary classification problem) given its TF-IDF features $(X)$.\n",
    "- $e$ is the base of the natural logarithm.\n",
    "- $\\beta_0$ is the intercept term of the logistic regression model.\n",
    "- $\\beta_1, \\beta_2, ..., \\beta_n$ are the coefficients associated with each TF-IDF feature $x_1, x_2, ..., x_n$, which the logistic regression model learns during training.\n",
    "\n",
    "This model predicts the probability that a given document belongs to a particular category (for instance, positive or negative sentiment) based on the weighted sum of its TF-IDF features, passed through a logistic (sigmoid) function to ensure the output is between 0 and 1.\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## (ii) Likelihood function for logistic regression model\n",
    "\n",
    "The likelihood function for a logistic regression model quantifies how probable the observed data $Y$ are, given the parameters of the model. For binary classification, where $y_i$ represents the binary outcome for the $i^{th}$ observation, and $X_i$ represents the feature vector for the $i^{th}$ observation, the likelihood function $L$ can be defined as the product of individual probabilities for each observation, assuming they are independent.\n",
    "\n",
    "Given:\n",
    "- $y_i$ is the binary outcome for the $i^{th}$ observation (0 or 1).\n",
    "- $X_i$ is the feature vector for the $i^{th}$ observation.\n",
    "- $\\beta$ is the vector of model parameters, including the intercept $\\beta_0$ and coefficients $\\beta_1, \\beta_2, ..., \\beta_n$.\n",
    "\n",
    "The probability of $y_i$ given $X_i$ and $\\beta$, denoted as $P(y_i | X_i; \\beta)$, is modeled by the logistic function:\n",
    "\n",
    "$$ P(y_i = 1 | X_i; \\beta) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_n x_{in})}} $$\n",
    "\n",
    "Thus, the likelihood function $L(\\beta)$ for $N$ observations is the product of the probabilities for each observation:\n",
    "\n",
    "$$ L(\\beta) = \\prod_{i=1}^{N} P(y_i | X_i; \\beta)^{y_i} (1 - P(y_i | X_i; \\beta))^{(1-y_i)} $$\n",
    "\n",
    "This can also be represented as:\n",
    "\n",
    "$$ L(\\beta) = \\prod_{i=1}^{N} \\left( \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_n x_{in})}} \\right)^{y_i} \\left(1 - \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_n x_{in})}} \\right)^{(1-y_i)} $$\n",
    "\n",
    "To facilitate computation, particularly for the purpose of parameter estimation via maximum likelihood estimation (MLE), it is common to work with the log-likelihood function, which is the logarithm of the likelihood function:\n",
    "\n",
    "$$ \\log L(\\beta) = \\sum_{i=1}^{N} \\left[ y_i \\log \\left( \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_n x_{in})}} \\right) + (1-y_i) \\log \\left(1 - \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_n x_{in})}} \\right) \\right] $$\n",
    "\n",
    "The goal of MLE is to find the values of $\\beta$ that maximize $\\log L(\\beta)$, thereby finding the parameter estimates that make the observed data most probable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "## (iii) Training the logistic regression model using Black-Box Model and measuring the performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusuion Matrix\n",
      " [[122  78]\n",
      " [ 58 142]]\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Accuracy:  66.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishan/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "# Predict the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "# Confusion matrix\n",
    "print('confusuion Matrix\\n',confusion_matrix(y_test, y_pred))\n",
    "print('\\n-----------------------------------------------------\\n')\n",
    "# Accuracy score\n",
    "print('Accuracy: ',accuracy_score(y_test, y_pred)*100,'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:  [[-0.12894731  0.12883174 -0.1418485  ...  0.11283745  0.11033777\n",
      "   0.16455937]]\n",
      "Intercept:  [0.07422433]\n",
      "Number of features:  4814\n"
     ]
    }
   ],
   "source": [
    "# print the coefficients\n",
    "print('Coefficients: ',model.coef_)\n",
    "print('Intercept: ',model.intercept_)\n",
    "print('Number of features: ',len(model.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4814 coefficients in the logistic regression model, which are learned during the training process. The coefficients can be accessed via model.coef_ and model.intercept_ attributes after fitting the model to the training data.\n",
    "\n",
    "Accuracy on Test Data: 66 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "### (iv) Training the logistic regression classifier by minimizing the negative log-likelihood function using a numerical optimization procedure: stochastic gradient descent(SGD) and Comparing with the coefficients obtained in step (iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_gradient(X, y, coefficients):\n",
    "    predictions = sigmoid(np.dot(X, coefficients))\n",
    "    errors = y - predictions\n",
    "    gradient = -np.dot(X.T, errors) / len(X)\n",
    "    return gradient\n",
    "\n",
    "def sgd_logistic_regression(X_train, y_train, learning_rate=0.01, epochs=1000):\n",
    "    coefficients = np.zeros(X_train.shape[1])\n",
    "    indices = np.arange(X_train.shape[0])  # Array of indices for X_train\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(indices)  # Shuffle indices for each epoch\n",
    "        for idx in indices:  # Iterate over shuffled indices\n",
    "            X_i = X_train[idx:idx+1]  # Use the shuffled index to access the sample\n",
    "            y_i = y_train[idx:idx+1]\n",
    "            gradient = compute_gradient(X_i, y_i, coefficients)\n",
    "            coefficients -= learning_rate * gradient\n",
    "    return coefficients\n",
    "\n",
    "# Train the model\n",
    "coefficients = sgd_logistic_regression(X_train.to_numpy(), y_train.to_numpy())\n",
    "\n",
    "# Predict the test data\n",
    "y_pred2 = sigmoid(np.dot(X_test.toarray(), coefficients))\n",
    "y_pred2 = np.round(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  67.75 %\n",
      "confusuion Matrix\n",
      " [[138  62]\n",
      " [ 67 133]]\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy\n",
    "print('Accuracy: ',accuracy_score(y_test, y_pred2)*100,'%')\n",
    "\n",
    "# Confusion matrix\n",
    "print('confusuion Matrix\\n',confusion_matrix(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Black-Box Model:  66.0 %\n",
      "Accuracy SGD Model:  67.75 %\n",
      "Intercept Black-Box Model:  [0.07422433]\n",
      "Intercept SGD Model:  -0.9063201496881317\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Black-Box Model</th>\n",
       "      <th>SGD Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaaaaah</td>\n",
       "      <td>-0.128947</td>\n",
       "      <td>-0.906320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaahhh</td>\n",
       "      <td>0.128832</td>\n",
       "      <td>0.729833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaarrggghhh</td>\n",
       "      <td>-0.141848</td>\n",
       "      <td>-0.587556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaaw</td>\n",
       "      <td>-0.195386</td>\n",
       "      <td>-1.408883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aag</td>\n",
       "      <td>-0.123730</td>\n",
       "      <td>-0.535092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aahhh</td>\n",
       "      <td>0.243232</td>\n",
       "      <td>1.504813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aaron</td>\n",
       "      <td>-0.208588</td>\n",
       "      <td>-1.136684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ab</td>\n",
       "      <td>-0.143054</td>\n",
       "      <td>-0.832974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>-0.114806</td>\n",
       "      <td>-0.498032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abc</td>\n",
       "      <td>0.253279</td>\n",
       "      <td>1.649064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>able</td>\n",
       "      <td>-0.387151</td>\n",
       "      <td>-1.722054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>abrasion</td>\n",
       "      <td>-0.223746</td>\n",
       "      <td>-0.940166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>abrowngirl</td>\n",
       "      <td>0.156679</td>\n",
       "      <td>0.849780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>absoloute</td>\n",
       "      <td>0.127169</td>\n",
       "      <td>0.728132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>absolutely</td>\n",
       "      <td>0.108906</td>\n",
       "      <td>0.350226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>abt</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.635373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ac</td>\n",
       "      <td>-0.118500</td>\n",
       "      <td>-0.608179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>academic</td>\n",
       "      <td>-0.102333</td>\n",
       "      <td>-0.443488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>accept</td>\n",
       "      <td>-0.264264</td>\n",
       "      <td>-1.452505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>accepting</td>\n",
       "      <td>-0.179556</td>\n",
       "      <td>-1.069020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Feature  Black-Box Model  SGD Model\n",
       "0       aaaaaah        -0.128947  -0.906320\n",
       "1        aaahhh         0.128832   0.729833\n",
       "2   aaarrggghhh        -0.141848  -0.587556\n",
       "3          aaaw        -0.195386  -1.408883\n",
       "4           aag        -0.123730  -0.535092\n",
       "5         aahhh         0.243232   1.504813\n",
       "6         aaron        -0.208588  -1.136684\n",
       "7            ab        -0.143054  -0.832974\n",
       "8     abandoned        -0.114806  -0.498032\n",
       "9           abc         0.253279   1.649064\n",
       "10         able        -0.387151  -1.722054\n",
       "11     abrasion        -0.223746  -0.940166\n",
       "12   abrowngirl         0.156679   0.849780\n",
       "13    absoloute         0.127169   0.728132\n",
       "14   absolutely         0.108906   0.350226\n",
       "15          abt         0.138371   0.635373\n",
       "16           ac        -0.118500  -0.608179\n",
       "17     academic        -0.102333  -0.443488\n",
       "18       accept        -0.264264  -1.452505\n",
       "19    accepting        -0.179556  -1.069020"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare the two models\n",
    "print('Accuracy Black-Box Model: ',accuracy_score(y_test, y_pred)*100,'%')\n",
    "print('Accuracy SGD Model: ',accuracy_score(y_test, y_pred2)*100,'%')\n",
    "\n",
    "#print the intercepts\n",
    "print('Intercept Black-Box Model: ',model.intercept_)\n",
    "print('Intercept SGD Model: ',coefficients[0])\n",
    "\n",
    "#make a df for the coefficients for black-box model and SGD model\n",
    "coefficients_df = pd.DataFrame({'Feature': X_train.columns, 'Black-Box Model': model.coef_[0], 'SGD Model': coefficients})\n",
    "\n",
    "\n",
    "#display all columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "#show head and tail of the coefficients_df\n",
    "\n",
    "coefficients_df.head(20)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusuion Matrix Black-Box Model\n",
      " [[122  78]\n",
      " [ 58 142]]\n",
      "confusuion Matrix SGD Model\n",
      " [[138  62]\n",
      " [ 67 133]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix for black-box model and SGD model\n",
    "print('confusuion Matrix Black-Box Model\\n',confusion_matrix(y_test, y_pred))\n",
    "print('confusuion Matrix SGD Model\\n',confusion_matrix(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "SGD implementation is quite basic and lacks several enhancements found in scikit-learn's version, such as regularization, adaptive learning rates, and convergence checks. \n",
    "\n",
    "This is the reason the performance and the resulting coefficients of this implementation are different from those obtained through Black-Box(scikit-learn's LogisticRegression) Model due to these and other optimizations.\n",
    "\n",
    "--------------------------\n",
    "**Model Outputs:**\n",
    "\n",
    "Accuracy Black-Box Model:  67.75 %\n",
    "\n",
    "Accuracy SGD Model:  67.75 %\n",
    "\n",
    "Intercept Black-Box Model:  [0.07422433]\n",
    "\n",
    "Intercept SGD Model:  -0.9063238446871242"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
